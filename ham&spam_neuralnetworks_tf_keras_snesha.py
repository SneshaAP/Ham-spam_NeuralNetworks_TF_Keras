# -*- coding: utf-8 -*-
"""Ham&spam_NeuralNetworks_TF_Keras - snesha

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OKMn5rovbuaE5DVMiT-rEs2fzf4-zbt4

### Approach to Keras:

1. Declare a Model
   1. Sequential: Sequence of Layers
   2. Functional: Multiple models with sequences of layers interconnected
2. Declare the Layers
   1. Embedding Layer, Dense Layers, Dropout Layers
   2. Tune the layers (Number of Neurons, act func, input_shape)
   3. Specify the output Layers
3. Compile the Model
   1. Optimizer (SGD, Adam)
   2. Loss Function (LogLoss/CrossEntropy, RMSE, MSE, MAE)
   3. Metrics for Evaluation (F1, Precision, Recall, Acc)
4. Fit the model
   1. X & Y
   2. Epochs
   3. Verbose = Log of the model run information
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils import to_categorical

data = pd.read_csv('/content/archive (6).zip', encoding='latin-1')

data.info()

"""drop the spill over columns"""

data = data.iloc[:,:2]

data.head()

"""Separate the messages and the labels"""



# Extract message text and labels
messages = data["v2"].tolist()  # Assuming a "text" column for messages
labels = data["v1"].tolist()  # Assuming a "label" column for ham/spam (0 or 1)

print(messages)

x_train, x_test, y_train, y_test = train_test_split(messages, labels, test_size =0.2, random_state = 42)

print(len(x_train))
print(len(x_test))
print(len(y_train))
print(len(y_test))

"""countvectorizer"""

vectorizer = CountVectorizer()
x_train_vectorized = vectorizer.fit_transform(x_train)
x_test_vectorized = vectorizer.transform(x_test)

# Convert labels to numerical values (0 for ham, 1 for spam)
y_train = [1 if labels == 'spam' else 0 for labels in y_train]
y_test = [1 if labels == 'spam' else 0 for labels in y_test]

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

"""Declear the model and layers"""

x_train_vectorized.shape[1]

model = Sequential()
model.add(Dense(32, activation='tanh', input_shape=(x_train_vectorized.shape[1],))) #Input Layer & HL1
model.add(Dropout(0.3))
model.add(Dense(16, activation = 'tanh')) #HL2
model.add(Dropout(0.3))
model.add(Dense(2, activation = 'sigmoid'))

model.summary()

"""Compile the model"""

model.compile(optimizer='adam', loss= 'binary_crossentropy', metrics = ['accuracy'])

"""Fit the model"""

x_train_vectorized.sort_indices()

history = model.fit(x_train_vectorized, y_train, epochs=10, batch_size=32, validation_data=(x_test_vectorized, y_test))

# Plot training & validation accuracy values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# make predictions on new data (optional)
new_message = "let us go out"
new_message_vectorized = vectorizer.transform([new_message])
prediction = model.predict(new_message_vectorized)[0][0]
if prediction > 0.5:
    print("Predicted Spam")
else:
    print("Predicted Ham")

print(model.predict(new_message_vectorized))

